

**Time Complexity:**
- `get_vocab(corpus)` ‚Üí O(n), n = total characters  
- `get_pairs(vocab)` ‚Üí O(m * k), m = number of words, k = avg word length  
- `merge_vocab(pair, vocab)` ‚Üí O(m * k)  
- Total for r merges ‚Üí O(r * m * k)  

**Space Complexity:** O(m * k) ‚Äî stores intermediate vocabularies  

> BPE efficiently handles rare words and reduces vocabulary size for NLP models.

---

## üõ† Technologies Used
- Python 3  
- `collections.defaultdict`  
- `re` module for regex-based tokenization  

---

## ‚ñ∂Ô∏è How to Run
```bash
python tokenization.py